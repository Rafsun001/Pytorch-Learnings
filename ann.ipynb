{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583591c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5dedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b08168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf563245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2749d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00b5940a",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a237be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepANN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=128)\n",
    "        \"\"\"\n",
    "        in_features: The number of input features (or the number of neurons from the    previous layer).\n",
    "\n",
    "        out_features: The number of output features (or the number of neurons in the current layer).\n",
    "\n",
    "        The layer performs a matrix multiplication between the input and the weight matrix, then adds a bias vector.\n",
    "\n",
    "        Output: The output of the layer is y = W * x + b, where x is the input, W is the weight matrix, and b is the bias vector\n",
    "        .\"\"\"\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64,bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=64)\n",
    "\n",
    "\n",
    "        # Third hidden layer\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32,bias=True)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=2,bias=True)  # For binary classification\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for output probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batch_norm(x)  \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Optimizer (Example, although you mentioned no training loop)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Loss function (for binary classification)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class or binary classification\n",
    "\n",
    "# Function to initialize the weights of the model using Xavier initialization\n",
    "def initialize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param)  # Xavier initialization for weights\n",
    "        elif 'bias' in name:\n",
    "            nn.init.zeros_(param)  # Zero initialization for bias\n",
    "\n",
    "model = DeepANN()\n",
    "\n",
    "# Initialize weights\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387a9bc",
   "metadata": {},
   "source": [
    "##### CNN 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f235633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer (Conv1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # Input: 1 channel, output: 32 channels\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Max Pooling Layer\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)  # BatchNorm after Conv1\n",
    "        \n",
    "        # Second Convolutional Layer (Conv2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)  # BatchNorm after Conv2\n",
    "        \n",
    "        # Fully Connected Layer (FC)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened 64*7*7 = 3136 input features\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output: 10 classes (for 10-class classification problem)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with a 50% chance to drop\n",
    "\n",
    "        # Softmax Activation\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batch_norm1(x)  # Apply BatchNorm\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Pass through the second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batch_norm2(x)  # Apply BatchNorm\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flatten the output of the conv layers to feed into the FC layers\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the output (Batch size, Flattened features)\n",
    "        # another way to flatter x = self.flatten(x)  \n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Softmax activation for classification probabilities\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize the weights of the model using Xavier initialization\n",
    "def initialize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param)  # Xavier initialization for weights\n",
    "        elif 'bias' in name:\n",
    "            nn.init.zeros_(param)  # Zero initialization for bias\n",
    "\n",
    "# Create the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Initialize weights\n",
    "initialize_weights(model)\n",
    "\n",
    "# Optimizer (Example)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Loss function (for multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# Example usage to verify the forward pass\n",
    "input_data = torch.randn(1, 1, 28, 28)  # Random input with 28x28 image and 1 channel (like MNIST)\n",
    "output = model(input_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abd028",
   "metadata": {},
   "source": [
    "##### CNN 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae18bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN3DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3DModel, self).__init__()\n",
    "\n",
    "        # First 3D Convolutional Layer (Conv1)\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # Input: 1 channel, output: 32 channels\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)  # 3D Max Pooling\n",
    "        self.batch_norm1 = nn.BatchNorm3d(32)  # BatchNorm after Conv1\n",
    "        \n",
    "        # Second 3D Convolutional Layer (Conv2)\n",
    "        self.conv2 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.batch_norm2 = nn.BatchNorm3d(64)  # BatchNorm after Conv2\n",
    "        \n",
    "        # Fully Connected Layer (FC)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7 * 7, 128)  # Flattened 64*7*7*7 = 25088 input features (example for 3D input)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output: 10 classes (for classification problem)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with a 50% chance to drop\n",
    "\n",
    "        # Softmax Activation\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the first 3D convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batch_norm1(x)  # Apply BatchNorm\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Pass through the second 3D convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batch_norm2(x)  # Apply BatchNorm\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flatten the output of the conv layers to feed into the FC layers\n",
    "        x = x.view(-1, 64 * 7 * 7 * 7)  # Flatten the output (Batch size, Flattened features)\n",
    "        \n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Softmax activation for classification probabilities\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize the weights of the model using Xavier initialization\n",
    "def initialize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param)  # Xavier initialization for weights\n",
    "        elif 'bias' in name:\n",
    "            nn.init.zeros_(param)  # Zero initialization for bias\n",
    "\n",
    "# Create the model\n",
    "model = CNN3DModel()\n",
    "\n",
    "# Initialize weights\n",
    "initialize_weights(model)\n",
    "\n",
    "# Optimizer (Example)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Loss function (for multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# Example usage to verify the forward pass\n",
    "input_data = torch.randn(1, 1, 20, 28, 28)  # Random input with 20 depth slices of 28x28 images (like video frames)\n",
    "output = model(input_data)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
