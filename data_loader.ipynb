{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325a15b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42b67937",
   "metadata": {},
   "source": [
    "# ---> torchvision <---  shikte hobe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3204bd",
   "metadata": {},
   "source": [
    "##### Normal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa08719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None): # It's like def function parameters style. Can pass n number\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Returns the total number of data samples in your dataset.\n",
    "        return len(self.data) # Returns the number of items in the dataset (i.e., how many data samples it contains).\n",
    "\n",
    "    def __getitem__(self, idx): # Returns a single data sample at a specific index idx along with its label (or target value).\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MyDataset(data, labels, transform=None)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\"\"\" \n",
    "    Parameters:\n",
    "        1. dataset: The Dataset object that holds your data.\n",
    "\n",
    "        2. batch_size: The number of samples to return in each batch.\n",
    "\n",
    "        3. shuffle: If True, data is shuffled before every epoch.\n",
    "\n",
    "        4. num_workers: The number of subprocesses to use for data loading. 0 means no additional subprocesses, and a higher number means parallel loading of data.\n",
    "\n",
    "        5. collate_fn: A function that allows you to customize how the data is merged into a batch.\n",
    "\n",
    "        6. drop_last: If True, drops the last batch if it is smaller than the specified batch size.\n",
    "\n",
    "        7. pin_memory: If True, data is copied to the GPU (useful when training with a GPU).\n",
    "\"\"\"\n",
    "\n",
    "for data_batch, label_batch in dataloader:\n",
    "    # Training loop or validation loop\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a480a",
   "metadata": {},
   "source": [
    "##### Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1adb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        # Read the data\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.features = self.data.drop(columns='target')  # Assuming 'target' is the label column\n",
    "        self.labels = self.data['target']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the features and label\n",
    "        sample = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)  # assuming classification task\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "dataset = TabularDataset(csv_file='data.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2efaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b556a1",
   "metadata": {},
   "source": [
    "##### Json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c635b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class JSONDataset(Dataset):\n",
    "    def __init__(self, json_file, transform=None):\n",
    "        # Read JSON data\n",
    "        with open(json_file, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text from the JSON file (assuming each entry has a 'text' key)\n",
    "        text = self.data[idx]['text']\n",
    "        sample = torch.tensor(text)  # You might want to use tokenization here\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "dataset = JSONDataset(json_file='data.json')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425e2ce",
   "metadata": {},
   "source": [
    "##### Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d443ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)  # List of subdirectory names (classes)\n",
    "        self.image_paths = []\n",
    "        \n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.image_paths.append((img_path, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolderDataset(root_dir='/images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88daf6ef",
   "metadata": {},
   "source": [
    "##### Image preprocessing task on loaded data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb05436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list): List of paths to the images.\n",
    "            labels (list): Corresponding labels for the images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply the transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations to apply\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),               # Resize the image\n",
    "    transforms.RandomHorizontalFlip(),           # Random horizontal flip for augmentation\n",
    "    transforms.ToTensor(),                       # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Instantiate dataset with transformations\n",
    "image_dataset = ImageDataset(image_paths=[\"path/to/image1.jpg\", \"path/to/image2.jpg\"], \n",
    "                             labels=[0, 1], transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(image_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700192a",
   "metadata": {},
   "source": [
    "##### Tabular data preprocessing taks on loaded data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c95a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file.\n",
    "            transform (callable, optional): A function/transform to apply to the data.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)  # Load data from CSV\n",
    "        self.features = self.data.drop(columns='target')  # Drop the target column\n",
    "        self.labels = self.data['target']\n",
    "        self.transform = transform\n",
    "\n",
    "        # Apply preprocessing (e.g., standardization) to features\n",
    "        self.scaler = StandardScaler()  # You can use more complex scalers if needed\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and label\n",
    "        sample = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        # Apply any transformations to the sample (if required)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "# Instantiate dataset\n",
    "dataset = TabularDataset(csv_file=\"data.csv\", transform=None)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eedfe0",
   "metadata": {},
   "source": [
    "##### LLM or text base data preprocessing on loaded data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): List of raw text samples.\n",
    "            labels (list, optional): List of labels for each text (for supervised learning).\n",
    "            tokenizer (transformers.Tokenizer): Tokenizer to convert text into tokens.\n",
    "            max_length (int): Maximum length for the tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer or BertTokenizer.from_pretrained('bert-base-uncased')  # Load default tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx] if self.labels else None\n",
    "\n",
    "        # Tokenize the text (convert text into token IDs, with padding/truncation)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens for BERT\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',    # Pad sequences to the max length\n",
    "            truncation=True,         # Truncate sequences longer than max_length\n",
    "            return_tensors='pt',     # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()  # Remove extra batch dimension (if any)\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Return tokenized data and corresponding label\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Example texts (list of sentences) and labels (for supervised tasks)\n",
    "texts = [\"This is the first sentence.\", \"Here is another sentence.\"]\n",
    "labels = [0, 1]  # Example labels for a classification task\n",
    "\n",
    "# Instantiate tokenizer and dataset\n",
    "dataset = TextDataset(texts=texts, labels=labels, tokenizer=None, max_length=32)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142d90f",
   "metadata": {},
   "source": [
    "#### Custom collete function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376fb2b",
   "metadata": {},
   "source": [
    "In PyTorch, a custom collate function is used when you want to customize how data is collated (i.e., combined into batches) from a dataset. By default, PyTorchâ€™s DataLoader will automatically stack tensors in a batch, but there are situations where you need to manipulate or modify the data in more complex ways before batching it, such as when the data is of different shapes, or when you want to apply special handling for certain data types (e.g., padding sequences for NLP tasks).\n",
    "\n",
    "What is the role of the collate function?\n",
    "    When the DataLoader iterates through the dataset, it retrieves samples one by one (from the __getitem__ method). The collate function combines these samples into a batch, which is then passed to the model. The default collate function simply stacks samples into a batch, but if your data has more complex structures, you might need to write a custom collate function.\n",
    "\n",
    "When to use a custom collate function?\n",
    "    You may need a custom collate function if:\n",
    "        Data with variable lengths: In NLP, for example, text sequences may have different lengths, and padding might be needed to make all sequences the same length.\n",
    "\n",
    "    Non-tensor data: \n",
    "        If you have non-tensor data (e.g., images with different sizes), you might need to handle them before batching.\n",
    "\n",
    "    Custom transformations: \n",
    "        When you want to apply special transformations or custom data processing during batching.\n",
    "\n",
    "    Handling complex data structures: \n",
    "        If your dataset consists of complex structures like dictionaries or tuples.\n",
    "\n",
    "How to create a custom collate function?\n",
    "A custom collate function takes in a list of samples (which are returned by the __getitem__ method of your dataset) and combines them into a single batch. It should return a batch that your model can work with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2621f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple Dataset class for NLP (example with variable-length sentences)\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Example data (list of sentences and labels)\n",
    "texts = [\"Hello\", \"How are you?\", \"Goodbye\"]\n",
    "labels = [0, 1, 0]\n",
    "\n",
    "# Define a custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that pads sentences to the same length and\n",
    "    returns a batch of input sequences and their labels.\n",
    "    \n",
    "    Args:\n",
    "        batch (list of tuples): Each element in the batch is a tuple (text, label).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A batch containing the padded input tensor and the label tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Find the maximum sentence length in the batch\n",
    "    max_length = max(len(sentence) for sentence, _ in batch)\n",
    "    \n",
    "    # Step 2: Pad all sentences to the same length\n",
    "    padded_texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for sentence, label in batch:\n",
    "        # Pad each sentence to the max length with 0s\n",
    "        padded_sentence = list(sentence) + [0] * (max_length - len(sentence))\n",
    "        padded_texts.append(padded_sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_tensor = torch.tensor(padded_texts)\n",
    "    label_tensor = torch.tensor(labels)\n",
    "    \n",
    "    return input_tensor, label_tensor\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = MyDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for inputs, labels in dataloader:\n",
    "    print(\"Input Tensor:\", inputs)\n",
    "    print(\"Label Tensor:\", labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
