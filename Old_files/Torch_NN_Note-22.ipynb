{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Type        | Task                       | Loss Function          |\n",
    "|-------------------|----------------------------|------------------------|\n",
    "| ANN (FCN)         | Multi-class Classification  | CrossEntropyLoss       |\n",
    "| CNN               | Binary Classification       | BCEWithLogitsLoss      |\n",
    "| RNN               | Regression                  | MSELoss                |\n",
    "| Any Model         | Robust Regression           | SmoothL1Loss           |\n",
    "| Any Model         | Distribution Learning       | KLDivLoss              |\n",
    "| Any Model         | Margin-based Classification | HingeEmbeddingLoss     |\n",
    "| Any Model         | Similarity Learning         | CosineEmbeddingLoss    |\n",
    "| Any Model         | Metric Learning             | TripletMarginLoss      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification\n",
    "\n",
    "criterion = nn.MSELoss()  # Suitable for regression\n",
    "\n",
    "criterion = nn.SmoothL1Loss()  # Replaces MSE for robustness\n",
    "\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "criterion = nn.HingeEmbeddingLoss()\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Activation Function | ANN | CNN | RNN |\n",
    "|--------------------|----|----|----|\n",
    "| **ReLU** (`nn.ReLU()`) | ✅ | ✅ | ❌ |\n",
    "| **Leaky ReLU** (`nn.LeakyReLU()`) | ✅ | ✅ | ❌ |\n",
    "| **Sigmoid** (`nn.Sigmoid()`) | ✅ | ✅ | ❌ |\n",
    "| **Tanh** (`nn.Tanh()`) | ❌ | ❌ | ✅ |\n",
    "| **Softmax** (`nn.Softmax()`) | ✅ | ❌ | ❌ |\n",
    "| **ELU** (`nn.ELU()`) | ✅ | ✅ | ❌ |\n",
    "| **SELU** (`nn.SELU()`) | ✅ | ❌ | ✅ |\n",
    "| **GELU** (`nn.GELU()`) | ✅ | ✅ | ✅ |\n",
    "| **Swish** (`nn.SiLU()`) | ✅ | ✅ | ❌ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu\n",
    "self.relu = nn.ReLU()\n",
    "\n",
    "# Sigmoid\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Leaky relu\n",
    "self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "# Tanh\n",
    "self.tanh = nn.Tanh()\n",
    "\n",
    "# Gelu\n",
    "self.gelu = nn.GELU()\n",
    "\n",
    "# ElU\n",
    "self.elu = nn.ELU()\n",
    "\n",
    "# SELU\n",
    "self.selu = nn.SELU()\n",
    "\n",
    "# Swish\n",
    "self.silu = nn.SiLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Technique | ANN | CNN | RNN | When to Use | Why to Use |\n",
    "|-----------|----|----|----|-------------|------------|\n",
    "| **Batch Normalization** (`nn.BatchNorm1d`, `nn.BatchNorm2d`) | ✅ | ✅ | ❌ | When training deep networks, especially CNNs and ANNs | Speeds up training and stabilizes learning by normalizing inputs per mini-batch |\n",
    "| **Layer Normalization** (`nn.LayerNorm`) | ✅ | ❌ | ✅ | When using RNNs and transformers | Normalizes across features instead of batches, making it useful for varying batch sizes |\n",
    "| **Instance Normalization** (`nn.InstanceNorm1d`, `nn.InstanceNorm2d`) | ❌ | ✅ | ❌ | When working with style transfer and image generation | Normalizes per sample, effective for tasks where batch statistics vary |\n",
    "| **Group Normalization** (`nn.GroupNorm`) | ❌ | ✅ | ❌ | When BatchNorm is ineffective due to small batch sizes | Normalizes across grouped channels instead of full batches |\n",
    "| **Dropout** (`nn.Dropout`) | ✅ | ❌ | ✅ | When overfitting occurs in fully connected layers and RNNs | Randomly disables neurons to prevent co-adaptation and improve generalization |\n",
    "| **Dropout2d** (`nn.Dropout2d`) | ❌ | ✅ | ❌ | When overfitting occurs in CNNs | Drops entire feature maps instead of individual neurons, improving feature independence |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "################### Normalization ###################\n",
    "#####################################################\n",
    "\n",
    "self.bn1 = nn.BatchNorm1d(128)  # Batch Normalization\n",
    "\n",
    "self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "self.in2 = nn.InstanceNorm2d(64)\n",
    "\n",
    "self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "self.gn = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "\n",
    "self.inorm = nn.InstanceNorm2d(32)\n",
    "\n",
    "#####################################################\n",
    "###################### Dropout ######################\n",
    "#####################################################\n",
    "# \n",
    "self.dropout = nn.Dropout(0.3)  # Dropout\n",
    "\n",
    "self.dropout3d = nn.Dropout3d(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Management\n",
    "\n",
    "tensor = tensor.to('cuda')\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "tensor = tensor.cuda()\n",
    "model = model.cuda()\n",
    "\n",
    "tensor = tensor.cpu()\n",
    "model = model.cpu()\n",
    "\n",
    "# Model Saving and Loading\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "state_dict = torch.load('model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "#b Gradient Calculation\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs)\n",
    "\n",
    "gradient = tensor.grad\n",
    "\n",
    "grads = torch.autograd.grad(loss, model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
