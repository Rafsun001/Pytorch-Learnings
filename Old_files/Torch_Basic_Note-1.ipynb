{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Creating Tensors in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating a tensor from a Python list (default dtype is inferred)\n",
    "tensor_1d = torch.tensor([1.0, 2.0, 3.0])  # 1D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2D tensor\n",
    "\n",
    "# 2. Creating a tensor from a NumPy array\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(np_array)  # Shares memory with NumPy array\n",
    "\n",
    "# 3. Checking tensor shape\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"Shape of tensor x:\", x.shape)  # Output: torch.Size([2, 2])\n",
    "\n",
    "# -------------------------------------------\n",
    "# Creating Tensors with Predefined Values\n",
    "# -------------------------------------------\n",
    "\n",
    "# 4. Creating an uninitialized tensor (values are uninitialized, may contain garbage values)\n",
    "tensor_empty = torch.empty(2, 3)  # 2 rows, 3 columns\n",
    "\n",
    "# 5. Creating tensors filled with zeros\n",
    "tensor_zeros = torch.zeros(3, 3)  # 3x3 tensor filled with 0s\n",
    "\n",
    "# 6. Creating tensors filled with ones\n",
    "tensor_ones = torch.ones(2, 2)  # 2x2 tensor filled with 1s\n",
    "\n",
    "# 7. Creating a tensor with random values\n",
    "tensor_random = torch.rand(4, 4)  # 4x4 tensor with random values (0 to 1)\n",
    "\n",
    "# 8. Setting manual seed for reproducibility (ensures the same random values each time)\n",
    "torch.manual_seed(132)  # Sets the seed for random number generation\n",
    "tensor_random_seeded = torch.rand(4, 4)  # 4x4 tensor with deterministic random values\n",
    "\n",
    "# -------------------------------------------\n",
    "# Creating Tensors Similar to an Existing Tensor\n",
    "# -------------------------------------------\n",
    "\n",
    "# 9. Creating an empty tensor with the same shape as another tensor\n",
    "tensor_empty_like = torch.empty_like(x)  # Same shape as x, uninitialized values\n",
    "\n",
    "# 10. Creating a zeros tensor with the same shape as another tensor\n",
    "tensor_zeros_like = torch.zeros_like(x)  # Same shape as x, filled with 0s\n",
    "\n",
    "# 11. Creating a ones tensor with the same shape as another tensor\n",
    "tensor_ones_like = torch.ones_like(x)  # Same shape as x, filled with 1s\n",
    "\n",
    "# 12. Creating a random tensor with the same shape as another tensor\n",
    "tensor_rand_like = torch.rand_like(x, dtype=torch.float32)  # Same shape as x, random values\n",
    "\n",
    "# Print some sample tensors to verify outputs\n",
    "print(\"Original Tensor:\\n\", x)\n",
    "print(\"Random Tensor with Seed:\\n\", tensor_random_seeded)\n",
    "print(\"Tensor of Zeros:\\n\", tensor_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tensor Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Checking Tensor Shape\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating a 2D tensor (matrix) with integer values\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Printing the shape of the tensor\n",
    "print(\"Shape of tensor x:\", x.shape)  # Output: torch.Size([2, 2])\n",
    "\n",
    "# -------------------------------------------\n",
    "# Checking Tensor Data Type\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating a 1D tensor with specified float32 data type\n",
    "x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "\n",
    "# Printing the data type of the tensor\n",
    "print(\"Data type of tensor x:\", x.dtype)  # Output: torch.float32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Tensor Reshaping and Manipulation\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. view(): Reshapes the tensor without changing its underlying data.\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y_view = x.view(3, 2)  # Reshapes to 3x2\n",
    "print(\"view() result:\\n\", y_view)\n",
    "\n",
    "# 2. reshape(): Similar to view(), but more flexible as it can handle non-contiguous tensors.\n",
    "y_reshape = x.reshape(3, 2)  # Reshapes to 3x2\n",
    "print(\"reshape() result:\\n\", y_reshape)\n",
    "\n",
    "# 3. squeeze(): Removes dimensions of size 1.\n",
    "x_squeeze_input = torch.tensor([[[1], [2], [3]]])  # Shape: (1,3,1)\n",
    "y_squeeze = x_squeeze_input.squeeze()  # Removes single-dimensional entries\n",
    "print(\"squeeze() result:\\n\", y_squeeze)\n",
    "\n",
    "# 4. unsqueeze(): Adds a dimension of size 1 at the specified position.\n",
    "x_unsqueeze_input = torch.tensor([1, 2, 3])  # Shape: (3,)\n",
    "y_unsqueeze = x_unsqueeze_input.unsqueeze(0)  # Adds a new dimension at position 0\n",
    "print(\"unsqueeze() result:\\n\", y_unsqueeze)\n",
    "\n",
    "# 5. transpose(): Swaps two dimensions in a tensor (useful for matrix operations).\n",
    "x_transpose_input = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y_transpose = x_transpose_input.transpose(0, 1)  # Swaps rows and columns\n",
    "print(\"transpose() result:\\n\", y_transpose)\n",
    "\n",
    "# 6. flatten(): Flattens the tensor into a 1D tensor.\n",
    "x_flatten_input = torch.tensor([[1, 2], [3, 4]])\n",
    "y_flatten = x_flatten_input.flatten()  # Flattens to a 1D tensor\n",
    "print(\"flatten() result:\\n\", y_flatten)\n",
    "\n",
    "# 7. stack(): Stacks multiple tensors along a new dimension.\n",
    "x1 = torch.tensor([1, 2])\n",
    "x2 = torch.tensor([3, 4])\n",
    "y_stack = torch.stack((x1, x2), dim=0)  # Stacks along a new dimension\n",
    "print(\"stack() result:\\n\", y_stack)\n",
    "\n",
    "# 8. cat(): Concatenates tensors along a given dimension.\n",
    "\n",
    "# Example: Concatenating along rows (dim=0)\n",
    "x1_cat = torch.tensor([[1, 2], [3, 4]])\n",
    "x2_cat = torch.tensor([[5, 6], [7, 8]])\n",
    "y_cat_rows = torch.cat((x1_cat, x2_cat), dim=0)  # Adds more rows\n",
    "print(\"Concatenation along rows:\\n\", y_cat_rows)\n",
    "\n",
    "# Example: Concatenating along columns (dim=1)\n",
    "y_cat_cols = torch.cat((x1_cat, x2_cat), dim=1)  # Adds more columns\n",
    "print(\"Concatenation along columns:\\n\", y_cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Casting and Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Tensor Casting & Type Conversion in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating a floating-point tensor\n",
    "x = torch.tensor([1.0, 2.0, 3.0])  # Default dtype is torch.float32\n",
    "\n",
    "# 2. Casting to an integer tensor\n",
    "x_int = x.int()  # Converts float tensor to int (torch.int32)\n",
    "# This will truncate the decimal part, resulting in [1, 2, 3]\n",
    "\n",
    "# 3. Casting back to a floating-point tensor\n",
    "x_float = x.float()  # Converts int tensor back to float (torch.float32)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Type Conversion in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 4. Creating a tensor with a specific data type (int32)\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.int32)  # Explicitly setting dtype to int32\n",
    "\n",
    "# 5. Converting to a different data type using `.to()`\n",
    "x_float32 = x.to(torch.float32)  # Converts int32 tensor to float32\n",
    "\n",
    "# -------------------------------------------\n",
    "# Printing and Verifying Data Types\n",
    "# -------------------------------------------\n",
    "print(\"Original Float Tensor (x):\", x)  # Should be an int32 tensor\n",
    "print(\"Converted to Integer (x_int):\", x_int)  # Should be int32\n",
    "print(\"Converted back to Float (x_float):\", x_float)  # Should be float32\n",
    "print(\"Tensor with Explicit dtype int32:\", x)  # Should be int32\n",
    "print(\"Converted to Float32 using .to():\", x_float32)  # Should be float32\n",
    "print(\"Data Type of x_float32:\", x_float32.dtype)  # Verifying the dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Indexing and Slicing in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating a 3x3 tensor\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9]])\n",
    "\n",
    "# 1. Selecting a specific element from the tensor\n",
    "element = x[1, 2]  # Selects the element at row index 1 and column index 2 (zero-based indexing)\n",
    "print(\"Selected Element:\", element)  # Output: tensor(6)\n",
    "\n",
    "# 2. Selecting an entire row\n",
    "row = x[0, :]  # Selects all elements in row index 0\n",
    "print(\"Selected Row:\", row)  # Output: tensor([1, 2, 3])\n",
    "\n",
    "# 3. Selecting an entire column\n",
    "column = x[:, 1]  # Selects all elements in column index 1\n",
    "print(\"Selected Column:\", column)  # Output: tensor([2, 5, 8])\n",
    "\n",
    "# 4. Selecting a sub-tensor (slicing)\n",
    "# Extracts rows from index 0 to 1 (exclusive of 2) and columns from index 1 to 2 (exclusive of 3)\n",
    "sub_tensor = x[0:2, 1:3]  \n",
    "print(\"Sliced Sub-Tensor:\\n\", sub_tensor)  \n",
    "# Output:\n",
    "# tensor([[2, 3],\n",
    "#         [5, 6]])\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Boolean Indexing in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating a 1D tensor\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 5. Boolean indexing: Selecting values greater than 3\n",
    "mask = x > 3  # Creates a boolean mask (True for values greater than 3)\n",
    "filtered = x[mask]  # Uses the mask to filter elements\n",
    "print(\"Filtered Elements (x > 3):\", filtered)  # Output: tensor([4, 5])\n",
    "\n",
    "# The boolean mask looks like: tensor([False, False, False, True, True])\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Fancy Indexing in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating another 1D tensor\n",
    "x = torch.tensor([10, 20, 30, 40, 50])\n",
    "\n",
    "# 6. Fancy indexing using a tensor of indices\n",
    "indices = torch.tensor([0, 3, 4])  # Selecting elements at index positions 0, 3, and 4\n",
    "selected = x[indices]  # Retrieves elements at those positions\n",
    "print(\"Selected Elements via Fancy Indexing:\", selected)  \n",
    "# Output: tensor([10, 40, 50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Math Operation on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Basic Arithmetic Operations\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Creating two 1D tensors\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise operations (default behavior in PyTorch)\n",
    "addition = x + y  # Element-wise addition\n",
    "multiplication = x * y  # Element-wise multiplication\n",
    "division = x / y  # Element-wise division\n",
    "\n",
    "# Equivalent alternative using tensor methods\n",
    "result_add = x.add(y)  # Addition\n",
    "result_sub = x.sub(y)  # Subtraction\n",
    "result_mul = x.mul(y)  # Multiplication\n",
    "result_div = x.div(y)  # Division\n",
    "\n",
    "# Integer division (dividing after multiplying by 100 and flooring the result)\n",
    "int_div = (x * 100) // 3\n",
    "\n",
    "# Modulo operation (remainder after integer division)\n",
    "mod_result = ((x * 100) // 3) % 2\n",
    "\n",
    "# Exponentiation (raising to power)\n",
    "power_result = x ** 2\n",
    "\n",
    "# Print results to verify\n",
    "print(\"Addition:\", addition)\n",
    "print(\"Multiplication:\", multiplication)\n",
    "print(\"Division:\", division)\n",
    "print(\"Integer Division:\", int_div)\n",
    "print(\"Modulo:\", mod_result)\n",
    "print(\"Power:\", power_result)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Absolute and Negative Value Operations\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "c = torch.tensor([1, -2, 3, -4])\n",
    "\n",
    "# Absolute values (converts all values to positive)\n",
    "abs_values = torch.abs(c)\n",
    "\n",
    "# Negating values (flipping signs)\n",
    "neg_values = torch.neg(c)\n",
    "\n",
    "print(\"Absolute Values:\", abs_values)\n",
    "print(\"Negated Values:\", neg_values)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Rounding and Clamping Operations\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "d = torch.tensor([1.9, 2.3, 3.7, 4.4])\n",
    "\n",
    "# Rounding to the nearest integer\n",
    "rounded = torch.round(d)\n",
    "\n",
    "# Ceiling (rounding up)\n",
    "ceiled = torch.ceil(d)\n",
    "\n",
    "# Floor (rounding down)\n",
    "floored = torch.floor(d)\n",
    "\n",
    "# Clamping values within a range (values < 2 become 2, values > 3 become 3)\n",
    "clamped = torch.clamp(d, min=2, max=3)\n",
    "\n",
    "print(\"Rounded Values:\", rounded)\n",
    "print(\"Ceil Values:\", ceiled)\n",
    "print(\"Floor Values:\", floored)\n",
    "print(\"Clamped Values:\", clamped)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Mathematical Functions\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "k = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Logarithm (natural log, base e)\n",
    "log_result = torch.log(k)\n",
    "\n",
    "# Exponential (e^x)\n",
    "exp_result = torch.exp(k)\n",
    "\n",
    "# Square root\n",
    "sqrt_result = torch.sqrt(k)\n",
    "\n",
    "# Sigmoid function (used in neural networks)\n",
    "sigmoid_result = torch.sigmoid(k)\n",
    "\n",
    "# Softmax function (used for probability distributions, summing to 1)\n",
    "softmax_result = torch.softmax(k, dim=0)\n",
    "\n",
    "# ReLU (Rectified Linear Unit, used in deep learning to remove negative values)\n",
    "relu_result = torch.relu(k)\n",
    "\n",
    "print(\"Log:\", log_result)\n",
    "print(\"Exp:\", exp_result)\n",
    "print(\"Sqrt:\", sqrt_result)\n",
    "print(\"Sigmoid:\", sigmoid_result)\n",
    "print(\"Softmax:\", softmax_result)\n",
    "print(\"ReLU:\", relu_result)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Reduction Operations (Sum, Mean, Min, Max, etc.)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Sum of all elements\n",
    "sum_all = x.sum()\n",
    "\n",
    "# Sum along columns (axis 0 → collapses rows)\n",
    "sum_col = torch.sum(x, dim=0)\n",
    "\n",
    "# Sum along rows (axis 1 → collapses columns)\n",
    "sum_row = torch.sum(x, dim=1)\n",
    "\n",
    "# Mean (average) of all elements\n",
    "mean_all = x.mean()\n",
    "\n",
    "# Mean along columns\n",
    "mean_col = torch.mean(x, dim=0)\n",
    "\n",
    "e = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Median (middle value)\n",
    "median_val = torch.median(e)\n",
    "\n",
    "# Maximum and Minimum Values\n",
    "max_val = torch.max(e)\n",
    "min_val = torch.min(e)\n",
    "\n",
    "# Product of all elements\n",
    "product = torch.prod(e)\n",
    "\n",
    "# Standard deviation (spread of values)\n",
    "std_dev = torch.std(e.float())  # Ensure float for precision\n",
    "\n",
    "# Variance (square of standard deviation)\n",
    "variance = torch.var(e.float())\n",
    "\n",
    "# Maximum and Minimum Value Indexes\n",
    "max_index = x.argmax()  # Index of the max value (flattened tensor)\n",
    "min_index = x.argmin()  # Index of the min value (flattened tensor)\n",
    "\n",
    "print(\"Sum:\", sum_all)\n",
    "print(\"Sum along Columns:\", sum_col)\n",
    "print(\"Sum along Rows:\", sum_row)\n",
    "print(\"Mean:\", mean_all)\n",
    "print(\"Mean along Columns:\", mean_col)\n",
    "print(\"Median:\", median_val)\n",
    "print(\"Max:\", max_val)\n",
    "print(\"Min:\", min_val)\n",
    "print(\"Product:\", product)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Variance:\", variance)\n",
    "print(\"Max Index:\", max_index)\n",
    "print(\"Min Index:\", min_index)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Matrix Multiplication\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix multiplication using @ operator\n",
    "matrix_mult1 = a @ b\n",
    "\n",
    "# Matrix multiplication using torch.matmul\n",
    "matrix_mult2 = torch.matmul(a, b)\n",
    "\n",
    "print(\"Matrix Multiplication (using @ operator):\\n\", matrix_mult1)\n",
    "print(\"Matrix Multiplication (using torch.matmul):\\n\", matrix_mult2)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# In-Place Operations (Modify Tensor Directly)\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# In-place addition (modifies x directly)\n",
    "x.add_(1.0)\n",
    "\n",
    "print(\"In-place Addition (x + 1):\", x)\n",
    "\n",
    "# Note:\n",
    "# - In-place operations modify the original tensor instead of creating a new one.\n",
    "# - They are denoted by an underscore (_), such as `add_`, `sub_`, `mul_`, etc.\n",
    "# - Using in-place operations can be risky in deep learning since it affects computation graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Matrix Operations in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating random integer matrices for demonstration\n",
    "f = torch.randint(size=(2, 3), low=0, high=10)  # 2x3 matrix with random values from 0 to 9\n",
    "g = torch.randint(size=(3, 2), low=0, high=10)  # 3x2 matrix with random values from 0 to 9\n",
    "\n",
    "# 2. Matrix multiplication (Dot product of matrices)\n",
    "# Uses the mathematical operation: result[i][j] = sum(A[i, k] * B[k, j])\n",
    "matrix_product = torch.matmul(f, g)  # Produces a (2x2) matrix\n",
    "\n",
    "# -------------------------------------------\n",
    "# Vector Operations in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 3. Defining two 1D vectors for dot product\n",
    "vector1 = torch.tensor([1, 2])  # Vector with 2 elements\n",
    "vector2 = torch.tensor([3, 4])  # Another vector with 2 elements\n",
    "\n",
    "# 4. Computing the dot product (scalar product) of two vectors\n",
    "# Formula: a·b = (1*3) + (2*4) = 3 + 8 = 11\n",
    "dot_product = torch.dot(vector1, vector2)  # Returns a scalar (single value)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Transposition of a Matrix\n",
    "# -------------------------------------------\n",
    "\n",
    "# 5. Transposing matrix `f` (swaps rows and columns)\n",
    "# torch.transpose(f, 0, 1) swaps dimension 0 (rows) with dimension 1 (columns)\n",
    "transposed_f = torch.transpose(f, 0, 1)  # Results in a (3x2) matrix\n",
    "\n",
    "# -------------------------------------------\n",
    "# Determinant and Inverse of a Square Matrix\n",
    "# -------------------------------------------\n",
    "\n",
    "# 6. Creating a square matrix (3x3) with random values for determinant and inverse\n",
    "h = torch.randint(size=(3, 3), low=0, high=10, dtype=torch.float32)  # Random 3x3 matrix\n",
    "\n",
    "# 7. Calculating the determinant of matrix `h`\n",
    "# The determinant is a scalar value that represents how the matrix scales space\n",
    "det_h = torch.det(h)  # Returns a single scalar value\n",
    "\n",
    "# 8. Computing the inverse of matrix `h`\n",
    "# Only possible if `h` is non-singular (determinant ≠ 0)\n",
    "# If the matrix is singular, this will raise an error\n",
    "if torch.det(h) != 0:  \n",
    "    inverse_h = torch.inverse(h)  # Computes the inverse of `h`\n",
    "else:\n",
    "    inverse_h = \"Matrix is singular, inverse does not exist.\"\n",
    "\n",
    "# -------------------------------------------\n",
    "# Printing Outputs for Verification\n",
    "# -------------------------------------------\n",
    "\n",
    "print(\"Matrix f:\\n\", f)\n",
    "print(\"Matrix g:\\n\", g)\n",
    "print(\"Matrix Multiplication (f * g):\\n\", matrix_product)\n",
    "\n",
    "print(\"\\nVector1:\", vector1)\n",
    "print(\"Vector2:\", vector2)\n",
    "print(\"Dot Product:\", dot_product.item())  # Convert to Python scalar\n",
    "\n",
    "print(\"\\nTransposed f:\\n\", transposed_f)\n",
    "\n",
    "print(\"\\nMatrix h:\\n\", h)\n",
    "print(\"Determinant of h:\", det_h.item())  # Convert to Python scalar\n",
    "print(\"Inverse of h:\\n\", inverse_h if isinstance(inverse_h, str) else inverse_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randint(size=(2,3), low=0, high=10)\n",
    "j = torch.randint(size=(2,3), low=0, high=10)\n",
    "\n",
    "# greater than\n",
    "i > j\n",
    "# less than\n",
    "i < j\n",
    "# equal to\n",
    "i == j\n",
    "# not equal to\n",
    "i != j\n",
    "# greater than equal to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Inplace Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# In-Place Addition of Tensors in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating two random tensors of shape (2,3)\n",
    "m = torch.rand(2, 3)  # Random values in the range [0, 1]\n",
    "n = torch.rand(2, 3)  # Another tensor with random values\n",
    "\n",
    "print(\"Tensor m (before addition):\\n\", m)\n",
    "print(\"Tensor n:\\n\", n)\n",
    "\n",
    "# 2. In-place addition using the `add_()` function\n",
    "# `add_()` modifies tensor `m` directly by adding `n` to it.\n",
    "m.add_(n)\n",
    "\n",
    "# `m` is now updated with the sum of `m` and `n`\n",
    "print(\"Tensor m (after in-place addition with n):\\n\", m)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Explanation of In-Place Operations in PyTorch\n",
    "# -------------------------------------------\n",
    "# - The underscore (`_`) at the end of `add_()` indicates an **in-place operation**.\n",
    "# - In-place operations directly modify the tensor instead of creating a new one.\n",
    "# - This means the original tensor `m` is updated, and no new tensor is created.\n",
    "# - Using in-place operations saves memory but should be used cautiously \n",
    "#   to avoid unintentional modifications.\n",
    "\n",
    "# Example of an out-of-place addition (does NOT modify `m`)\n",
    "m_copy = m + n  # Creates a new tensor instead of modifying `m`\n",
    "print(\"New tensor created by out-of-place addition:\\n\", m_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Tensor Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Tensor Cloning vs Assignment in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating a tensor\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# 2. Cloning a tensor (creates an independent copy)\n",
    "y = x.clone()  # y is a new tensor with the same values as x\n",
    "\n",
    "# 3. Modifying the cloned tensor\n",
    "y[0, 0] = 99  # Changes only y, not x\n",
    "\n",
    "# Printing results\n",
    "print(\"Original Tensor x (Unchanged):\\n\", x)  # x remains the same\n",
    "print(\"Cloned Tensor y (Modified):\\n\", y)  # y is modified\n",
    "\n",
    "# -------------------------------------------\n",
    "# Assignment vs Cloning in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 4. Creating a random tensor\n",
    "a = torch.rand(2, 3)  # A 2x3 tensor with random values\n",
    "\n",
    "# 5. Assigning `a` to `b` (both reference the same memory)\n",
    "b = a  # No new tensor is created; b points to the same data as a\n",
    "\n",
    "# 6. Modifying `b` will also modify `a`\n",
    "b[0, 0] = 42  # Since b and a share memory, this also affects a\n",
    "\n",
    "# Printing results\n",
    "print(\"Tensor a (Modified due to assignment):\\n\", a)  # a is modified\n",
    "print(\"Tensor b (Same as a, since they share memory):\\n\", b)  # b reflects the same modification\n",
    "\n",
    "# -------------------------------------------\n",
    "# Explanation:\n",
    "# -------------------------------------------\n",
    "# - `clone()` creates an independent copy of the tensor, meaning changes in the cloned tensor (y)\n",
    "#   do not affect the original tensor (x).\n",
    "# - `b = a` does NOT create a copy. Instead, `b` is just another reference to `a`, meaning any changes\n",
    "#   to `b` also change `a` since they share the same memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Contiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Contiguous and Non-Contiguous Tensors in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# Creating a 2D tensor\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# Transposing the tensor\n",
    "y = x.t()  # .t() transposes the tensor (swaps rows and columns)\n",
    "\n",
    "# Checking if the tensor is contiguous in memory\n",
    "print(\"Is transposed tensor contiguous?\", y.is_contiguous())  # Expected: False\n",
    "\n",
    "# -------------------------------------------\n",
    "# Why is the transposed tensor non-contiguous?\n",
    "# -------------------------------------------\n",
    "# In PyTorch, a tensor's data is stored as a contiguous block in memory.\n",
    "# When we transpose a tensor, it does NOT rearrange the data in memory.\n",
    "# Instead, it changes the way the data is accessed (viewed), making it non-contiguous.\n",
    "# A non-contiguous tensor cannot be directly used in some PyTorch operations.\n",
    "\n",
    "# -------------------------------------------\n",
    "# Making the tensor contiguous\n",
    "# -------------------------------------------\n",
    "\n",
    "# Calling .contiguous() forces a copy of the tensor into a contiguous memory layout\n",
    "y_contiguous = y.contiguous()\n",
    "\n",
    "# Checking if the new tensor is contiguous\n",
    "print(\"Is contiguous version contiguous?\", y_contiguous.is_contiguous())  # Expected: True\n",
    "\n",
    "# -------------------------------------------\n",
    "# Verifying the difference\n",
    "# -------------------------------------------\n",
    "\n",
    "# Checking memory layout of the original and transposed tensors\n",
    "print(\"Original Tensor:\\n\", x)\n",
    "print(\"Transposed Tensor (Non-Contiguous):\\n\", y)\n",
    "print(\"Contiguous Transposed Tensor:\\n\", y_contiguous)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Why is making it contiguous important?\n",
    "# -------------------------------------------\n",
    "# Many PyTorch operations require contiguous tensors for efficiency.\n",
    "# If a tensor is non-contiguous, some operations (like reshaping) might fail or require an implicit conversion.\n",
    "# Using .contiguous() ensures that the tensor can be used seamlessly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Chunk and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------\n",
    "# Splitting Tensors in PyTorch\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Creating a 1D tensor\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(\"Original Tensor:\", x)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Splitting Tensor into Equal Chunks\n",
    "# -------------------------------------------\n",
    "\n",
    "# 2. Splitting the tensor into 3 equal chunks\n",
    "# `torch.chunk()` splits the tensor into the specified number of equal chunks (if possible).\n",
    "# If the tensor size is not evenly divisible, the last chunk may be smaller.\n",
    "chunks = torch.chunk(x, 3)\n",
    "\n",
    "# Printing each chunk\n",
    "print(\"\\nTensor split into 3 chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\", chunk)\n",
    "\n",
    "# Expected Output:\n",
    "# Chunk 1: tensor([1, 2])\n",
    "# Chunk 2: tensor([3, 4])\n",
    "# Chunk 3: tensor([5, 6])\n",
    "\n",
    "# -------------------------------------------\n",
    "# Splitting Tensor into Parts of a Given Size\n",
    "# -------------------------------------------\n",
    "\n",
    "# 3. Splitting the tensor into parts of size 2\n",
    "# `torch.split()` splits the tensor into smaller tensors of the specified size.\n",
    "# If the tensor size is not a multiple of the given size, the last split may be smaller.\n",
    "splits = torch.split(x, 2)\n",
    "\n",
    "# Printing each split\n",
    "print(\"\\nTensor split into parts of size 2:\")\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Split {i+1}:\", split)\n",
    "\n",
    "# Expected Output:\n",
    "# Split 1: tensor([1, 2])\n",
    "# Split 2: tensor([3, 4])\n",
    "# Split 3: tensor([5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
