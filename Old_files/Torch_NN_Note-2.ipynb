{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Type        | Task                       | Loss Function          |\n",
    "|-------------------|----------------------------|------------------------|\n",
    "| ANN (FCN)         | Multi-class Classification  | CrossEntropyLoss       |\n",
    "| CNN               | Binary Classification       | BCEWithLogitsLoss      |\n",
    "| RNN               | Regression                  | MSELoss                |\n",
    "| Any Model         | Robust Regression           | SmoothL1Loss           |\n",
    "| Any Model         | Distribution Learning       | KLDivLoss              |\n",
    "| Any Model         | Margin-based Classification | HingeEmbeddingLoss     |\n",
    "| Any Model         | Similarity Learning         | CosineEmbeddingLoss    |\n",
    "| Any Model         | Metric Learning             | TripletMarginLoss      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cross-Entropy Loss\n",
    "# ------------------------------------\n",
    "# Used for multi-class classification problems.\n",
    "# This loss function is applied when the model outputs raw logits (unscaled scores).\n",
    "# The loss calculates the difference between the predicted logits and the actual class index \n",
    "# using a softmax function to normalize the logits into a probability distribution.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# Model output: Raw logits of shape (batch_size, num_classes).\n",
    "# Target: Integer class indices (0 to num_classes-1) for each input.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 2. Binary Cross-Entropy Loss with Logits (BCEWithLogitsLoss)\n",
    "# ------------------------------------\n",
    "# Used for binary classification tasks, where the model outputs raw logits.\n",
    "# This loss combines a sigmoid activation function with binary cross-entropy loss \n",
    "# to directly calculate the loss on raw model output without needing separate sigmoid activation.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification\n",
    "# Model output: Raw logits of shape (batch_size, 1).\n",
    "# Target: Binary class (0 or 1).\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 3. Mean Squared Error Loss (MSELoss)\n",
    "# ------------------------------------\n",
    "# Typically used for regression tasks. This loss measures the squared difference \n",
    "# between the predicted and target continuous values.\n",
    "# The goal is to minimize the squared error between predicted and actual values.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.MSELoss()  # Suitable for regression\n",
    "# Model output: Continuous values (e.g., predicted house price, temperature).\n",
    "# Target: Continuous values representing ground truth.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 4. Smooth L1 Loss\n",
    "# ------------------------------------\n",
    "# A robust version of the Mean Squared Error (MSELoss) that is less sensitive to outliers.\n",
    "# It behaves like MSE for small differences and like Mean Absolute Error (MAE) for large differences.\n",
    "# This loss is useful in scenarios where data has outliers or you want robustness in the model's learning.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.SmoothL1Loss()  # Replaces MSE for robustness\n",
    "# Model output: Continuous values.\n",
    "# Target: Continuous values (e.g., ground truth).\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 5. Kullback-Leibler Divergence Loss (KLDivLoss)\n",
    "# ------------------------------------\n",
    "# Used to measure how one probability distribution diverges from a second, expected distribution.\n",
    "# It’s often applied in generative models like Variational Autoencoders (VAE) or in tasks involving \n",
    "# distribution learning. This loss calculates the divergence between the predicted distribution \n",
    "# and the true probability distribution.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')  # Calculates KL divergence with batch mean reduction\n",
    "# Model output: Log-probabilities (log(p)) of predicted distribution.\n",
    "# Target: True probability distribution (p).\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 6. Hinge Embedding Loss\n",
    "# ------------------------------------\n",
    "# Primarily used for binary classification tasks, often with margin-based models (e.g., Support Vector Machines).\n",
    "# The loss function computes the margin between the predicted value and the true label.\n",
    "# It penalizes predictions that are on the wrong side of the margin, encouraging correct predictions \n",
    "# that are confidently far from the decision boundary.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.HingeEmbeddingLoss()\n",
    "# Model output: Predictions as -1 or 1 (binary classes).\n",
    "# Target: True class as -1 or 1.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 7. Cosine Embedding Loss\n",
    "# ------------------------------------\n",
    "# Commonly used in similarity learning, where the goal is to minimize the angle \n",
    "# between two vectors. This loss encourages similar samples (vectors) to be closer in angle and \n",
    "# dissimilar ones to be farther apart. It’s often used for tasks like face verification.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "# Model output: Embedding vectors (e.g., face embeddings, text embeddings).\n",
    "# Target: 1 for similar pairs, -1 for dissimilar pairs.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 8. Triplet Margin Loss\n",
    "# ------------------------------------\n",
    "# Used in metric learning to learn an embedding space where the distance between \n",
    "# similar samples is smaller than the distance between dissimilar samples by a specified margin.\n",
    "# This loss is typically applied in tasks such as face recognition, where the model learns \n",
    "# to distinguish between positive and negative pairs of embeddings.\n",
    "\n",
    "# Example Usage:\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "# Model output: Embeddings of three samples: anchor, positive, and negative.\n",
    "# Target: No explicit target needed; loss function works based on relative distances between embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bias is added in each layer. So if you want to add a bias in one layer then you have make it bias paramter value Ture and if you don't want then make it False.\n",
    "\"\"\"\n",
    "# Example in liner layer:\n",
    "self.fc1 = nn.Linear(28*28, 128, bias=True)  # Bias enabled\n",
    "\n",
    "# Example Convolution layer\n",
    "self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "# Example RNN layer\n",
    "self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, bias=True)\n",
    "\n",
    "\"\"\"\n",
    "Just make it false if you don't want to use it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Activation Function | ANN | CNN | RNN |\n",
    "|--------------------|----|----|----|\n",
    "| **ReLU** (`nn.ReLU()`) | ✅ | ✅ | ❌ |\n",
    "| **Leaky ReLU** (`nn.LeakyReLU()`) | ✅ | ✅ | ❌ |\n",
    "| **Sigmoid** (`nn.Sigmoid()`) | ✅ | ✅ | ❌ |\n",
    "| **Tanh** (`nn.Tanh()`) | ❌ | ❌ | ✅ |\n",
    "| **Softmax** (`nn.Softmax()`) | ✅ | ❌ | ❌ |\n",
    "| **ELU** (`nn.ELU()`) | ✅ | ✅ | ❌ |\n",
    "| **SELU** (`nn.SELU()`) | ✅ | ❌ | ✅ |\n",
    "| **GELU** (`nn.GELU()`) | ✅ | ✅ | ✅ |\n",
    "| **Swish** (`nn.SiLU()`) | ✅ | ✅ | ❌ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ReLU (Rectified Linear Unit)\n",
    "# ------------------------------------\n",
    "# The ReLU activation function is one of the most widely used activation functions.\n",
    "# It outputs zero for all negative inputs and outputs the input itself for all positive inputs.\n",
    "# It helps with avoiding the vanishing gradient problem by allowing positive gradients to flow \n",
    "# through the network during backpropagation.\n",
    "\n",
    "self.relu = nn.ReLU() \n",
    "# Example: input = -2, output = 0; input = 3, output = 3.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 2. Sigmoid\n",
    "# ------------------------------------\n",
    "# The Sigmoid function outputs values in the range (0, 1), making it useful for binary classification tasks.\n",
    "# It \"squashes\" the output into a probability-like range. \n",
    "# However, it can suffer from the vanishing gradient problem when values are too close to 0 or 1.\n",
    "\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "# Example: input = -3, output = 0.047; input = 2, output = 0.881.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 3. Leaky ReLU\n",
    "# ------------------------------------\n",
    "# Leaky ReLU is a variant of ReLU. Instead of outputting zero for negative values, \n",
    "# it allows a small, non-zero gradient (controlled by the alpha parameter) to flow for negative inputs.\n",
    "# This helps with the \"dying ReLU\" problem, where ReLU neurons may get stuck during training.\n",
    "\n",
    "self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "# Example: input = -2, output = -0.2; input = 3, output = 3.\n",
    "# The slope for negative inputs is 0.1 (default).\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 4. Tanh (Hyperbolic Tangent)\n",
    "# ------------------------------------\n",
    "# Tanh is similar to the sigmoid function but its range is (-1, 1) rather than (0, 1).\n",
    "# It is zero-centered, which helps in some cases, especially with gradient flow during backpropagation.\n",
    "# However, it can still suffer from the vanishing gradient problem.\n",
    "\n",
    "self.tanh = nn.Tanh()\n",
    "# Example: input = -2, output = -0.964; input = 2, output = 0.964.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 5. GELU (Gaussian Error Linear Unit)\n",
    "# ------------------------------------\n",
    "# GELU is a smooth approximation of the ReLU activation that allows for non-linearity and smoother training.\n",
    "# It has the benefits of both ReLU and Tanh, with a non-zero output for negative values, but more stability during training.\n",
    "# It is often used in transformer models and other deep architectures.\n",
    "\n",
    "self.gelu = nn.GELU()\n",
    "# Example: input = -2, output = -0.046; input = 2, output = 0.977.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 6. ELU (Exponential Linear Unit)\n",
    "# ------------------------------------\n",
    "# The ELU activation function outputs the input for positive values and applies an exponential function for negative values.\n",
    "# It helps with avoiding the vanishing gradient problem and can accelerate learning in some cases.\n",
    "# It has a small negative output for negative inputs, unlike ReLU which outputs 0.\n",
    "\n",
    "self.elu = nn.ELU()\n",
    "# Example: input = -2, output = -0.135; input = 3, output = 3.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 7. SELU (Scaled Exponential Linear Unit)\n",
    "# ------------------------------------\n",
    "# SELU is a self-normalizing activation function that automatically scales its output \n",
    "# to have zero mean and unit variance, making it useful for deep networks.\n",
    "# It is an extension of ELU but with scaled parameters to enable self-normalization.\n",
    "\n",
    "self.selu = nn.SELU()\n",
    "# Example: input = -2, output = -1.757; input = 2, output = 2.742.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 8. Swish (SiLU: Sigmoid Linear Unit)\n",
    "# ------------------------------------\n",
    "# Swish, or SiLU, is a newer activation function that is a smooth combination of sigmoid and linear functions.\n",
    "# It can help improve model performance, particularly in deep neural networks, by being more flexible than ReLU.\n",
    "# It is known to offer better performance on a variety of tasks, as it does not suffer from dead neurons like ReLU.\n",
    "\n",
    "self.silu = nn.SiLU()\n",
    "# Example: input = -2, output = -0.119; input = 2, output = 1.762.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Technique | ANN | CNN | RNN | When to Use | Why to Use |\n",
    "|-----------|----|----|----|-------------|------------|\n",
    "| **Batch Normalization** (`nn.BatchNorm1d`, `nn.BatchNorm2d`) | ✅ | ✅ | ❌ | When training deep networks, especially CNNs and ANNs | Speeds up training and stabilizes learning by normalizing inputs per mini-batch |\n",
    "| **Layer Normalization** (`nn.LayerNorm`) | ✅ | ❌ | ✅ | When using RNNs and transformers | Normalizes across features instead of batches, making it useful for varying batch sizes |\n",
    "| **Instance Normalization** (`nn.InstanceNorm1d`, `nn.InstanceNorm2d`) | ❌ | ✅ | ❌ | When working with style transfer and image generation | Normalizes per sample, effective for tasks where batch statistics vary |\n",
    "| **Group Normalization** (`nn.GroupNorm`) | ❌ | ✅ | ❌ | When BatchNorm is ineffective due to small batch sizes | Normalizes across grouped channels instead of full batches |\n",
    "| **Dropout** (`nn.Dropout`) | ✅ | ❌ | ✅ | When overfitting occurs in fully connected layers and RNNs | Randomly disables neurons to prevent co-adaptation and improve generalization |\n",
    "| **Dropout2d** (`nn.Dropout2d`) | ❌ | ✅ | ❌ | When overfitting occurs in CNNs | Drops entire feature maps instead of individual neurons, improving feature independence |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "################### Normalization ###################\n",
    "#####################################################\n",
    "\n",
    "# 1. Batch Normalization (1D)\n",
    "# ------------------------------------\n",
    "# Batch Normalization is used to normalize the activations of the neurons in a mini-batch.\n",
    "# It helps the network train faster and stabilizes the learning process by reducing internal covariate shift.\n",
    "# Here, it normalizes over a batch of 1D data (e.g., sequence data or flat vectors).\n",
    "self.bn1 = nn.BatchNorm1d(128)  # Batch Normalization for 1D input with 128 features.\n",
    "# This normalizes each feature (column) across the mini-batch.\n",
    "\n",
    "# 2. Batch Normalization (2D)\n",
    "# ------------------------------------\n",
    "# This performs batch normalization for 2D data, which is typically used for image data or other 2D input.\n",
    "# The normalization is done per channel (i.e., across the height and width of the image), and the model learns scale and shift parameters for each channel.\n",
    "self.bn2 = nn.BatchNorm2d(32)  # Batch Normalization for 2D input with 32 channels.\n",
    "# The input is expected to be of shape (batch_size, num_channels, height, width).\n",
    "\n",
    "# 3. Instance Normalization (2D)\n",
    "# ------------------------------------\n",
    "# Instance Normalization normalizes each individual sample (rather than a batch of samples) in the batch.\n",
    "# This is especially useful in tasks like style transfer and image generation, where each image needs to be normalized independently.\n",
    "self.in2 = nn.InstanceNorm2d(64)  # Instance Normalization for 2D input with 64 channels.\n",
    "# It normalizes each image (instance) separately, useful for styles where every image is treated individually.\n",
    "\n",
    "# 4. Layer Normalization\n",
    "# ------------------------------------\n",
    "# Layer Normalization normalizes the activations of each layer, rather than each mini-batch. \n",
    "# It’s applied across the features for each sample independently, and it's often used in models like RNNs.\n",
    "self.ln = nn.LayerNorm(hidden_size)  # Layer Normalization for hidden_size features.\n",
    "# The input is expected to be a tensor where each sample is normalized across the features (not the batch dimension).\n",
    "\n",
    "# 5. Group Normalization\n",
    "# ------------------------------------\n",
    "# Group Normalization divides the channels into groups and normalizes each group separately.\n",
    "# It’s an alternative to batch normalization, and it works well when the batch size is small or when batch statistics are unreliable.\n",
    "self.gn = nn.GroupNorm(num_groups=8, num_channels=64)  # Group Normalization with 8 groups and 64 channels.\n",
    "# This is particularly useful in scenarios with smaller batches, like segmentation tasks.\n",
    "\n",
    "# 6. Instance Normalization (2D) - Duplicate\n",
    "# ------------------------------------\n",
    "# Another instance normalization layer for 2D data, similar to the earlier one.\n",
    "# Instance normalization helps in domains like generative models, especially for style transfer tasks.\n",
    "self.inorm = nn.InstanceNorm2d(32)  # Instance Normalization for 2D input with 32 channels.\n",
    "# It normalizes each instance separately, similar to the previous InstanceNorm2d layer.\n",
    "\n",
    "#####################################################\n",
    "###################### Dropout ######################\n",
    "#####################################################\n",
    "\n",
    "# 1. Dropout\n",
    "# ------------------------------------\n",
    "# Dropout is a regularization technique that randomly sets a fraction of the input units to 0 during training.\n",
    "# This prevents the model from overfitting by reducing reliance on specific neurons, forcing the network to learn more robust features.\n",
    "self.dropout = nn.Dropout(0.3)  # Dropout with 30% probability of setting units to zero.\n",
    "# This helps prevent overfitting by randomly deactivating 30% of the neurons during training.\n",
    "\n",
    "# 2. 3D Dropout\n",
    "# ------------------------------------\n",
    "# 3D Dropout is an extension of dropout, but it's applied to 3D data like video or 3D image data.\n",
    "# It works the same way as regular dropout, but instead of operating on 2D planes, it applies to the 3D volume of the input.\n",
    "self.dropout3d = nn.Dropout3d(0.3)  # 3D Dropout with 30% probability.\n",
    "# Useful for regularizing 3D CNNs in tasks like 3D object detection or video processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Method | ANN | CNN | RNN | When to Use | Why to Use |\n",
    "|--------|----|----|----|-------------|------------|\n",
    "| **Xavier (Glorot) Initialization** (`xavier_uniform_`, `xavier_normal_`) | ✅ | ❌ | ✅ | When using **Tanh/Sigmoid** activations | Maintains variance across layers |\n",
    "| **Kaiming (He) Initialization** (`kaiming_uniform_`, `kaiming_normal_`) | ❌ | ✅ | ❌ | When using **ReLU/LeakyReLU** activations | Helps deeper networks converge faster |\n",
    "| **Orthogonal Initialization** (`orthogonal_`) | ❌ | ❌ | ✅ | For **RNNs, LSTMs, GRUs** | Preserves information across time steps |\n",
    "| **Uniform Initialization** (`uniform_`) | ✅ | ✅ | ✅ | General purpose | Randomized initialization within a fixed range |\n",
    "| **Normal Initialization** (`normal_`) | ✅ | ✅ | ✅ | General purpose | Gaussian distribution for weights |\n",
    "| **Constant Initialization** (`constant_`) | ✅ | ✅ | ✅ | Used for bias terms | Sets weights/biases to fixed values |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom work emebdding train krte hobe\n",
    "\n",
    "token_embedding = nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "################# Device Management #######################\n",
    "###########################################################\n",
    "\n",
    "# Move tensor to the default GPU device (CUDA device).\n",
    "tensor = tensor.to('cuda')\n",
    "# Move the model to the default GPU device. This is necessary before training starts if a GPU is available, as it speeds up computations.\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Another way to move the tensor to GPU. This defaults to the first GPU if multiple GPUs are available.\n",
    "tensor = tensor.cuda()\n",
    "# Move the model to the GPU. Equivalent to using .to('cuda') but specifically calls out CUDA.\n",
    "model = model.cuda()\n",
    "\n",
    "# Move the tensor back to CPU. This is useful for operations that need to be performed on the CPU or for compatibility reasons.\n",
    "tensor = tensor.cpu()\n",
    "# Move the model back to CPU. Useful when you need to save the model or perform CPU-only operations.\n",
    "model = model.cpu()\n",
    "\n",
    "##############################################################\n",
    "############### Model Saving and Loading  ####################\n",
    "##############################################################\n",
    "\n",
    "# Save the model's state dictionary. This contains the model's weights, not the entire model structure.\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load a model state dictionary from the disk. This does not load the full model but only the parameters.\n",
    "state_dict = torch.load('model.pth')\n",
    "\n",
    "# Load the parameters into the existing model structure. This is necessary because torch.load does not load the model structure.\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "#################################################################\n",
    "#################### Gradient Calculation #######################\n",
    "#################################################################\n",
    "\n",
    "# Calculate gradients of the loss with respect to model parameters. This is used during the backward pass of training.\n",
    "loss.backward()\n",
    "\n",
    "# Context manager that disables gradient calculations. Gradients are not needed during inference, which makes operations faster and reduces memory usage.\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs)\n",
    "\n",
    "# Access the gradient of a specific tensor. Useful for debugging or for custom operations involving gradients.\n",
    "gradient = tensor.grad\n",
    "\n",
    "# Compute the gradients of 'loss' with respect to the parameters of the model. This function returns the gradients and can be used when you need more control over how gradients are calculated or when you need to retrieve them explicitly.\n",
    "grads = torch.autograd.grad(loss, model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layers\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "\n",
    "nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "\n",
    "# Pooling Layers\n",
    "nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)\n",
    "\n",
    "# Fully Connected (Dense) Layers\n",
    "nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "# Padding Layers\n",
    "nn.ZeroPad2d(padding)\n",
    "\n",
    "# Flatten\n",
    "self.flatten = nn.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Layer\n",
    "\n",
    "# GRU Layer \n",
    "\n",
    "# LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential\n",
    "\n",
    "# nn.ModuleList\n",
    "\n",
    "# nn.ModuleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
